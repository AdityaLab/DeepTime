import pandas as pd, numpy as np
import argparse
import pickle
import ast
import os
import numpy as np
import pickle
from sklearn.metrics import accuracy_score, f1_score

def accuracy_score(y_true, y_pred):
    assert len(y_true) == len(y_pred)
    return np.sum(y_true == y_pred) / len(y_true)


def convert_pkl(raw_pickle, answer_prefix="Answer Choice "):
    """
    Convert raw pickle files saved in run.py to answers to each individual question

    raw_pickle [dict]: Python dict read from raw pickle files
    answer_prefix [str]: Prefix for answer matching, it should match the template in prompt.py
    """
    results = {}

    def extract_ans(ans_str, search_substring):
        # Split the string into lines
        lines = ans_str.split("\n")

        for line in lines:
            # Check if the line starts with the specified substring
            if line.startswith(search_substring):
                # If it does, add it to the list of extracted rows
                return line[len(search_substring) :].strip()
        return "ERROR"  # Answer not found

    for k, v in raw_pickle.items():
        if k.startswith("["):  # Skip token_usage
            qns_idx = ast.literal_eval(k)
            for idx, qn_idx in enumerate(qns_idx):
                results[qn_idx] = extract_ans(
                    v, f"{answer_prefix}{idx+1}:"
                )  # We start with question 1
    return results


def cal_metrics_V0(
    EXP_NAME, test_df, classes, class_desp, show_error=True, bootstrap=1000
):
    """
    Calculate accuracy from model responses

    EXP_NAME [str]: experiment name which should match the pkl file generated by run.py
    test_df [pandas dataframe]: dataframe for test cases
    classes [list of str]: names of categories for classification, and this should match tbe columns of test_df and demo_df.
    class_desp [list of str]: category descriptions for classification, and these are the actual options sent to the model
    show_error [bool]: whether parsing errors will be printed
    bootstrap [int]: number of replicates for bootstrapping standard deviation
    """

    with open(f"{EXP_NAME}.pkl", "rb") as f:
        results = pickle.load(f)
    results = convert_pkl(
        results
    )  # Convert the batched results into individual answers

    test_df = test_df[classes]

    category_labels = test_df.idxmax(axis=1)
    index_labels = category_labels.map(class_to_idx)

    num_errors = 0
    labels, preds = [], []
    for i in test_df.itertuples():
        if (i.Index not in results) or (results[i.Index].startswith("ERROR")):
            num_errors += 1
            if show_error:
                print(i.Index, f"answer not found")
            continue

        pred_text = results[i.Index]
        pred_idx = len(classes)
        for idx, class_name in enumerate(class_desp):
            if class_name.lower() in pred_text.lower():
                if pred_idx == len(classes):
                    pred_idx = idx
                else:
                    pred_idx = -1  # Multiple predictions found
        if pred_idx >= 0:
            labels.append(index_labels.loc[i.Index])
            preds.append(pred_idx)
        else:
            if show_error:
                print(
                    i.Index, f"multiple predictions found. raw response = {pred_text}"
                )
            num_errors += 1

    print(f"{EXP_NAME} In total {num_errors} errors len = {len(labels)}")

    y_true = np.array(labels)
    y_pred = np.array(preds)
    ori_acc = 100 * accuracy_score(y_true, y_pred)
    accs = []
    for boot in range(bootstrap):
        idx = np.random.choice(len(y_true), size=len(y_true), replace=True)
        accs.append(100 * accuracy_score(y_true[idx], y_pred[idx]))
    print(f"Accuracy: {ori_acc:.2f} +- {np.std(accs):.2f}")

def cal_metrics(
    EXP_NAME, test_df, classes, class_desp, show_error=True, bootstrap=1000
):
    """
    Calculate accuracy and F1 score from model responses

    EXP_NAME [str]: experiment name which should match the pkl file generated by run.py
    test_df [pandas dataframe]: dataframe for test cases
    classes [list of str]: names of categories for classification, and this should match tbe columns of test_df and demo_df.
    class_desp [list of str]: category descriptions for classification, and these are the actual options sent to the model
    show_error [bool]: whether parsing errors will be printed
    bootstrap [int]: number of replicates for bootstrapping standard deviation
    """

    with open(f"{EXP_NAME}.pkl", "rb") as f:
        results = pickle.load(f)
    results = convert_pkl(
        results
    )  # Convert the batched results into individual answers

    test_df = test_df[classes]

    category_labels = test_df.idxmax(axis=1)
    index_labels = category_labels.map(class_to_idx)

    num_errors = 0
    labels, preds = [], []
    for i in test_df.itertuples():
        if (i.Index not in results) or (results[i.Index].startswith("ERROR")):
            num_errors += 1
            if show_error:
                print(i.Index, f"answer not found")
            continue

        pred_text = results[i.Index]
        pred_idx = len(classes)
        for idx, class_name in enumerate(class_desp):
            if class_name.lower() in pred_text.lower():
                if pred_idx == len(classes):
                    pred_idx = idx
                else:
                    pred_idx = -1  # Multiple predictions found
        if pred_idx >= 0:
            labels.append(index_labels.loc[i.Index])
            preds.append(pred_idx)
        else:
            if show_error:
                print(
                    i.Index, f"multiple predictions found. raw response = {pred_text}"
                )
            num_errors += 1

    print(f"{EXP_NAME} In total {num_errors} errors len = {len(labels)}")

    y_true = np.array(labels)
    y_pred = np.array(preds)
    ori_acc = 100 * accuracy_score(y_true, y_pred)
    ori_f1 = 100 * f1_score(y_true, y_pred, average='weighted')
    accs = []
    f1s = []
    for boot in range(bootstrap):
        idx = np.random.choice(len(y_true), size=len(y_true), replace=True)
        accs.append(100 * accuracy_score(y_true[idx], y_pred[idx]))
        f1s.append(100 * f1_score(y_true[idx], y_pred[idx], average='weighted'))
    print(f"Accuracy: {ori_acc:.2f} +- {np.std(accs):.2f}")
    print(f"F1 Score: {ori_f1:.2f} +- {np.std(f1s):.2f}")
if __name__ == "__main__":
    # Initialize the parser
    parser = argparse.ArgumentParser(description="Experiment script.")
    # Adding the arguments
    parser.add_argument(
        "--dataset",
        type=str,
        required=True,
        default="UCMerced",
        help="The dataset to use",
    )
    parser.add_argument(
        "--model",
        type=str,
        required=False,
        default="gpt-4o-mini-2024-07-18",
        help="The model to use",
    )
    parser.add_argument(
        "--location",
        type=str,
        required=False,
        default="us-central1",
        help="The location for the experiment",
    )
    parser.add_argument(
        "--num_shot_per_class",
        type=int,
        default=3,
        required=False,
        help="The number of shots per class",
    )
    parser.add_argument(
        "--num_qns_per_round",
        type=int,
        required=False,
        default=1,
        help="The number of questions asked each time",
    )
    parser.add_argument(
        "--question",
        type=str,
        required=False,
        default="What is in the image above",
        help="The question to ask",
    )
    parser.add_argument(
        "--name",
        type=str,
        required=False,
        default="",
        help="The name",
    )
    parser.add_argument(
        "--modal",
        type=str,
        required=False,
        default="V",
        help="L: language, V: vision, LV: language and vision",
    ) 
    parser.add_argument(
        "--similar_use",
        type=int,
        required=False,
        default=0,
        help="The similar use",
    )
    parser.add_argument(
        "--similar_num",
        type=int,
        required=False,
        default=-1,
        help="The similar number",
    )
    # Parsing the arguments
    args = parser.parse_args()

    # Using the arguments
    dataset_name = args.dataset
    model = args.model
    location = args.location
    num_shot_per_class = args.num_shot_per_class
    num_qns_per_round = args.num_qns_per_round

    # Read the two dataframes for the dataset
    demo_df = pd.read_csv(f"./Dataset/{dataset_name}/demo.csv", index_col=0)
    test_df = pd.read_csv(f"./Dataset/{dataset_name}/test.csv", index_col=0)

    classes = list(demo_df.columns)  # classes for classification
    class_desp = classes  # The actual list of options given to the model. If the column names are informative enough, we can just use them.
    class_to_idx = {class_name: idx for idx, class_name in enumerate(classes)}
    name=args.name
    # 原有的 EXP_NAME 定义
    if args.similar_use==0:
        num_shots=num_shot_per_class*len(classes)
    else:
        num_shots=args.similar_num
    if args.similar_use==0:
        EXP_NAME = f"{dataset_name}_{model}model_{args.modal}modal_{num_shots}randomshot_{num_qns_per_round}preround"

    else:
        EXP_NAME = f"{dataset_name}_{model}model_{args.modal}modal_{num_shots}similarshot_{num_qns_per_round}preround"
    folder_path = f"./{name}/"

    os.makedirs(folder_path, exist_ok=True)

    EXP_NAME = os.path.join(folder_path, EXP_NAME)
    cal_metrics(EXP_NAME, test_df, classes, class_desp)
